{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from csv import DictReader, DictWriter\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "kTOKENIZER = TreebankWordTokenizer()\n",
    "import nltk\n",
    "import sys, gzip, codecs\n",
    "reader = codecs.getreader('utf8')\n",
    "writer = codecs.getwriter('utf8')\n",
    "def morphy_stem(word):\n",
    "    \"\"\"\n",
    "    Simple stemmer\n",
    "    \"\"\"\n",
    "    stem = wn.morphy(word)\n",
    "    if stem:\n",
    "        return stem.lower()\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def features(text):\n",
    "    d = defaultdict(int)\n",
    "    for ii in kTOKENIZER.tokenize(text):\n",
    "        d[morphy_stem(ii)] += 1\n",
    "    return d\n",
    "def prepfile(fh, code):\n",
    "  if type(fh) is str:\n",
    "    fh = open(fh, code)\n",
    "  ret = gzip.open(fh.name, code if code.endswith(\"t\") else code+\"t\") if fh.name.endswith(\".gz\") else fh\n",
    "  if sys.version_info[0] == 2:\n",
    "    if code.startswith('r'):\n",
    "      ret = reader(fh)\n",
    "    elif code.startswith('w'):\n",
    "      ret = writer(fh)\n",
    "    else:\n",
    "      sys.stderr.write(\"I didn't understand code \"+code+\"\\n\")\n",
    "      sys.exit(1)\n",
    "  return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be 1\n",
      "winter 1\n",
      "when 1\n",
      ". 1\n",
      "wild 2\n",
      "wind 1\n"
     ]
    }
   ],
   "source": [
    "b1=features('When winter winds were wild wild.')\n",
    "for k, v in b1.items():\n",
    "    print k, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['When', 'winter', 'winds', 'were', 'wild', 'wild', '.']\n",
      "[('When', 'WRB'), ('winter', 'NN'), ('winds', 'NNS'), ('were', 'VBD'), ('wild', 'JJ'), ('wild', 'JJ'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "print kTOKENIZER.tokenize('When winter winds were wild wild.')\n",
    "print nltk.pos_tag(kTOKENIZER.tokenize('When winter winds were wild wild.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "len_dic = {}\n",
    "frequent  = {}\n",
    "for i in prepfile('train.tsv', 'r'):\n",
    "    #print kTOKENIZER.tokenize(i.strip('\\n').split('\\t')[1])\n",
    "    for ii in kTOKENIZER.tokenize(i.strip('\\n').split('\\t')[1]):\n",
    "        if ii == ',':\n",
    "            print ii\n",
    "            cnt += 1\n",
    "    if cnt>10:\n",
    "        break\n",
    "        #if i not in frequent:\n",
    "            #frequent[i] = 1\n",
    "        #else:\n",
    "            #frequent[i] += 1\n",
    "        #if len(i) not in len_dic:\n",
    "            #len_dic[len(i)] = 1\n",
    "        #else:\n",
    "            #len_dic[len(i)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 17\n",
      "1 4958\n",
      "2 4543\n",
      "3 5415\n",
      "4 6365\n",
      "5 3997\n",
      "6 2363\n",
      "7 1506\n",
      "8 927\n",
      "9 431\n",
      "10 195\n",
      "11 66\n",
      "12 36\n",
      "13 24\n",
      "14 12\n",
      "15 4\n",
      "16 2\n",
      "17 1\n"
     ]
    }
   ],
   "source": [
    "print len(len_dic)\n",
    "for k in len_dic.keys():\n",
    "    print k, len_dic[k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4916\n",
      "[(u'fawn', 1), (u'Savage', 1), (u'pardon', 1), (u'ladybird', 1), (u'Redeemer', 1), (u'decays', 1), (u'Lean', 1), (u'four', 1), (u'rain-clouds', 1), (u'hanging', 1)]\n",
      "[(u'my', 412), (u'and', 434), (u'And', 440), (u'to', 465), (u'I', 482), (u'of', 482), (u';', 532), (u'.', 619), (u'the', 757), (u',', 2346)]\n",
      "[(u',', 2346)]\n"
     ]
    }
   ],
   "source": [
    "ff = sorted([(k, v) for k, v in frequent.items()], key=lambda x: x[1])\n",
    "print len(ff)\n",
    "print ff[:10]\n",
    "print ff[-10:]\n",
    "print ff[4915:4916]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491.6\n",
      "983.2\n",
      "1474.8\n",
      "1966.4\n",
      "2458.0\n",
      "2949.6\n",
      "3441.2\n",
      "3932.8\n",
      "4424.4\n",
      "4916.0\n"
     ]
    }
   ],
   "source": [
    "fd = {}\n",
    "prev = 0\n",
    "for i in xrange(1,11):\n",
    "    print 4916*(i/float(10))\n",
    "    curr = int(4916*(i/float(10)))    \n",
    "    fd[i] = [x[0] for x in ff[prev : curr]]\n",
    "    prev = curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdd = {}\n",
    "for k, v in fd.items():\n",
    "    for w in v:\n",
    "        if w not in fdd:\n",
    "            fdd[w] = k\n",
    "        else:\n",
    "            print w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump( fdd, open( \"frequent_data\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fawn 1\n"
     ]
    }
   ],
   "source": [
    "for k, v in fdd.items():\n",
    "    print k, v\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(sequence, n):\n",
    "    # sequence: sequence of words, list\n",
    "    # n: degree of ngrams\n",
    "\n",
    "    iter_seq = iter(sequence)\n",
    "    history = []\n",
    "    # make first n-1 elements ready\n",
    "    while n > 1:\n",
    "        history.append(next(iter_seq))\n",
    "        n -= 1\n",
    "    # print(history)\n",
    "    # print(sequence)\n",
    "\n",
    "    # add Nth element, yield, and get rid of first element\n",
    "    # repeat the process\n",
    "    for item in iter_seq:\n",
    "        # print(item)\n",
    "        history.append(item)\n",
    "        # print(tuple(history))\n",
    "        yield tuple(history)\n",
    "        del history[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngd = {}\n",
    "frequent  = {}\n",
    "for i in prepfile('train.tsv', 'r'):\n",
    "    #print kTOKENIZER.tokenize(i.strip('\\n').split('\\t')[1])\n",
    "    for ii in ngrams(kTOKENIZER.tokenize(i.strip('\\n').split('\\t')[1]), 2):\n",
    "        if ii not in ngd:\n",
    "            ngd[ii] = 1\n",
    "        else:\n",
    "            ngd[ii] += 1\n",
    "        #if i not in frequent:\n",
    "            #frequent[i] = 1\n",
    "        #else:\n",
    "            #frequent[i] += 1\n",
    "        #if len(i) not in len_dic:\n",
    "            #len_dic[len(i)] = 1\n",
    "        #else:\n",
    "            #len_dic[len(i)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20040"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ngd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20040\n",
      "[((u'revealed', u'how'), 1), ((u'born', u'of'), 1), ((u'Forget', u','), 1), ((u'verse', u'to'), 1), ((u'earthly', u'cares'), 1), ((u'loves', u'I'), 1), ((u'the', u'well'), 1), ((u'murmured', u'with'), 1), ((u'Make', u'war'), 1), ((u'Against', u'thy'), 1)]\n",
      "[((u'of', u'the'), 32), ((u'my', u'love'), 32), ((u'love', u','), 32), ((u'to', u'the'), 33), ((u'on', u'the'), 33), ((u'O', u','), 34), ((u'thee', u','), 35), ((u'me', u','), 46), ((u'in', u'the'), 54), ((u',', u'and'), 103)]\n",
      "[((u',', u'and'), 103)]\n"
     ]
    }
   ],
   "source": [
    "ngf = sorted([(k, v) for k, v in ngd.items()], key=lambda x: x[1])\n",
    "print len(ngf)\n",
    "print ngf[:10]\n",
    "print ngf[-10:]\n",
    "print ngf[20039:20040]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2004\n",
      "2004 4008\n",
      "4008 6012\n",
      "6012 8016\n",
      "8016 10020\n",
      "10020 12024\n",
      "12024 14028\n",
      "14028 16032\n",
      "16032 18036\n",
      "18036 20040\n"
     ]
    }
   ],
   "source": [
    "ngff= {}\n",
    "ngff_2= {}\n",
    "prev = 0\n",
    "for i in xrange(1,11):\n",
    "    # print 20040*(i/float(10))\n",
    "    curr = int(20040*(i/float(10)))    \n",
    "    ngff[i] = [x[0] for x in ngf[prev : curr]]\n",
    "    for ii in [x[0] for x in ngf[prev : curr]]:\n",
    "        ngff_2[ii] = i\n",
    "    print prev, curr\n",
    "    prev = curr "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2004\n",
      "2 2004\n",
      "3 2004\n",
      "4 2004\n",
      "5 2004\n",
      "6 2004\n",
      "7 2004\n",
      "8 2004\n",
      "9 2004\n",
      "10 2004\n"
     ]
    }
   ],
   "source": [
    "for k, v in ngff.items():\n",
    "    print k, len(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20040"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump( ngff_2, open( \"frequent_ngram\", \"wb\" ) )\n",
    "len(ngff_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
